{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLP+/4CinDoneZnlYAHD1w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Cc7iA5prFgi1"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import concurrent.futures\n","import pandas as pd\n","import numpy as np\n","import requests\n","import time\n","import json\n","import os\n","import re"]},{"cell_type":"code","source":["clean_data_path = '/content/drive/MyDrive/Kschool_TFM/datasets'\n","raw_data_path = '/content/drive/MyDrive/Kschool_TFM/raw_data'\n","config_path = '/content/drive/MyDrive/Kschool_TFM/config'"],"metadata":{"id":"ur3dR5_8Qflp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class idealista_scraper:\n","\n","    def __init__(self, url:str, config_path:str, raw_data_path:str, clean_data_path:str) -> None:\n","        self._IDEALISTA_HOSTNAME = 'https://www.idealista.com'\n","        self.api_endpoint = 'http://async.scraperapi.com/jobs'\n","        self._url = url\n","        self.config_path = config_path\n","        with open(f'{self.config_path}{os.sep}config.json',encoding='utf-8',mode='r') as f:\n","            config = json.load(f)\n","            self.api_key = config['idealista_api_key']\n","            self.raw_data_path = raw_data_path\n","            self.clean_data_path = clean_data_path\n","\n","             \n","    def _proxy_requests(self, url:str) -> dict:\n","        '''\n","        Use the scraping API to access the URL given as input.\n","        Returns a dict with the scraping response.\n","\n","        args:\n","        - url: string containing the url to scrape\n","        '''\n","\n","        json_api_config = {\n","            'apiKey':self.api_key,\n","            'url':url\n","            }\n","        \n","        response_job = requests.post(self.api_endpoint, json=json_api_config)\n","        for _ in range(30): # limits to 10 the number of attempts to retrieve the response\n","            if requests.get(url = response_job.json()['statusUrl']).json()['status'] == \"finished\":\n","                break        \n","            time.sleep(5)\n","        \n","        response = requests.get(url = response_job.json()['statusUrl'])\n","        return response.json()['response']\n","        \n","    def _links_from_breadcrumb(self, thisUrl:str) -> list:\n","        '''\n","        Gets an area url as input and returns a list of links for all \n","        the areas (district or subdistrict) with 1800 houses or less\n","        If it finds areas with more than 1800 houses, it runs itself\n","        recursively.\n","        The data is obtained from the breadcrumb element of the HTML page\n","\n","        Args:\n","        - thisUrl (str): the url of the area to look into\n","\n","        '''\n","        \n","        print(f'starting from {thisUrl}')\n","        response = self._proxy_requests(thisUrl)\n","        if response['statusCode'] != 200:\n","            raise f'Could not retrieve data for {thisUrl}'\n","        soup = BeautifulSoup(response['body'],'html.parser')\n","        \n","        current_level_links = [x for x in soup.select('li.breadcrumb-dropdown-element.highlighted a')]\n","        current_level_house_numbers = [int(x.text.replace('.','')) for x in soup.select('li.breadcrumb-dropdown-element.highlighted .breadcrumb-navigation-sidenote') if x.text]\n","\n","        current_level_list = list(zip(current_level_links,current_level_house_numbers))\n","\n","        # Separating areas by the number of houses\n","        more_than_1800_houses = [x for x in current_level_list if x[1] > 1800]\n","        print(f'found {len(more_than_1800_houses)} areas with more than 1800 houses')\n","\n","        ready_for_scraping = [x for x in current_level_list if x[1] <= 1800]\n","        print(f'found {len(ready_for_scraping)} areas with less than 1800 houses')\n","        \n","        # Recursively searching through areas with more than 1.800 houses\n","        for area,_ in more_than_1800_houses:\n","            area_url = self._IDEALISTA_HOSTNAME+area.get('href')\n","            print(f'recursion over: {area_url}')\n","            areas = self._links_from_breadcrumb(area_url)\n","            ready_for_scraping.extend(areas)\n","        \n","        return ready_for_scraping\n","\n","    def get_areas_df(self) -> None:\n","        '''\n","        Has to be run first. It creates a list of urls for each area\n","        that can be found as a subarea of the url given as input.\n","        Writes a new class property named \"areas_df\" (pandas DataFrame)\n","\n","        '''\n","\n","        areas_list = [[x[0].text,x[0].get('href'),x[1]] for x in self._links_from_breadcrumb(self._url)]\n","        self.areas_df = pd.DataFrame(areas_list, columns=['area_name','area_url','n_houses']).sort_values(by='n_houses')\n","        self.areas_df = self.areas_df.reset_index().drop('index',axis=1)\n","        self.areas_df['page'] = 1\n","        self.areas_df['done'] = False\n","        self.areas_df.to_csv(f'{self.raw_data_path}{os.sep}areas_df.csv', index=False)\n","        print('Created area_df.csv')\n","\n","    def generate_properties_links_df(self) -> None:\n","        '''\n","        Has to be run after get_areas_df as it needs areas_df to work.\n","        It iterates through each area stored in areas_df DataFrame, and extracts\n","        houses' links from each page of the area (district or subdistrict)\n","\n","        It creates a new class property \"properties_links_df\"\n","\n","        '''\n","\n","        if not hasattr(self,'areas_df'):\n","            try:\n","                print(f'No areas_df dataframe. Try to read from CSV.')\n","                self.areas_df = pd.read_csv(f'{self.raw_data_path}{os.sep}areas_df.csv')\n","            except:\n","                raise 'Please run get_areas_df first'\n","            \n","        \n","        print(f'Areas found: {self.areas_df.shape[0]}')\n","        \n","        \n","        with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n","            executor.map(\n","                self._generate_single_area_property_links,\n","                self.areas_df[self.areas_df['done']==False].iterrows()\n","                )\n","            \n","\n","        self.properties_links_df = pd.read_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv')\n","\n","    def _generate_single_area_property_links(self, thisArea) -> None:\n","        '''\n","        Used do retrieve the properties' links for a specific area\n","        \n","        '''\n","        thisIndex, area = thisArea\n","        page = int(area[\"page\"])\n","        print(f'Area {area[\"area_name\"]}, page {page}')\n","        path = f\"{self._IDEALISTA_HOSTNAME}{area['area_url']}pagina-{str(page)}.htm?ordenado-por=fecha-publicacion-asc\"\n","        print(f\"Getting properties\\' url for {area['area_name']}\")\n","\n","        while True:\n","            print(f'Path is {path}')\n","            response = self._proxy_requests(path)\n","            if response['statusCode'] != 200:\n","                raise f\"Cannot retrieve data for page {page} of {area['area_name']}\"\n","            soup = BeautifulSoup(response['body'],'html.parser')\n","            \n","            # extracting links from page and creating a 3 columns df\n","            thisPageLinks = [[area['area_name'],self._IDEALISTA_HOSTNAME+x.get('href'),False] for x in soup.select('a.item-link')]\n","            thisPageLinks_df = pd.DataFrame(thisPageLinks, columns=['area_name','property_link','done'])\n","\n","            # create or concat data to properties_links_df\n","            if hasattr(self,'properties_links_df'):\n","                self.properties_links_df = pd.concat([self.properties_links_df, thisPageLinks_df])\n","            else:\n","                try:\n","                    self.properties_links_df = pd.concat(pd.read_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv'),thisPageLinks_df)\n","                except:\n","                    self.properties_links_df = thisPageLinks_df.copy()\n","            \n","            header = not os.path.exists(f'{self.raw_data_path}{os.sep}properties_links_df.csv')\n","            thisPageLinks_df.to_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv', mode='a', index=False, header=header)\n","\n","            print(f'Property links added for {area[\"area_name\"]} page {page}')\n","\n","            # if there is a next page\n","            next_page = soup.select_one('.pagination .next a')\n","            if bool(next_page):\n","                # done with this page\n","                path = self._IDEALISTA_HOSTNAME+next_page.get('href')\n","                # storing next page on areas_df in case this breaks\n","                page += 1\n","                self.areas_df.at[thisIndex,\"page\"] = page\n","                print(f'Next page: {path}, which is number {page}')\n","                self.areas_df.to_csv(f'{self.raw_data_path}{os.sep}areas_df.csv', index=False)\n","            else:\n","                # done with this area\n","                self.areas_df.at[thisIndex,\"done\"] = True\n","                print(f\"all properties\\' links from {area['area_name']} have been extracted\")\n","                break  \n","\n","    def get_properties_data(self) -> None:\n","        '''\n","        Has to run after generate_properties_links_df. It takes the properties links, \n","        access them and dump the html as text files locally.\n","\n","        '''\n","\n","        if not hasattr(self,'properties_links_df'):\n","            try:\n","                self.properties_links_df = pd.read_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv').drop_duplicates()\n","            except:\n","                raise 'Cannot find properties\\' links'\n","\n","        print(f'Properties\\' links found: {self.properties_links_df[self.properties_links_df[\"done\"]==False].shape[0]}.')\n","\n","        with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n","            executor.map(\n","                self._dump_single_property_data,\n","                self.properties_links_df[self.properties_links_df['done']==False].iterrows()\n","                )\n","\n","    def _dump_single_property_data(self,thisRow) -> None:\n","        thisIndex, row = thisRow\n","        property_id = row['property_link'].split('/')[-2]\n","        print(f\"{property_id}: retrieving data\")\n","        response = self._proxy_requests(row[\"property_link\"])\n","        print(f'{property_id}: scraping returned a response')\n","        if response['statusCode'] == 404:\n","            print(f'{property_id}: property not found {row[\"property_link\"]}')\n","            self.properties_links_df = self.properties_links_df.drop(thisIndex)\n","            self.properties_links_df.to_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv',index=False)\n","            return None        \n","\n","        with open(f'{self.raw_data_path}{os.sep}properties{os.sep}{property_id}','w') as f:\n","            f.write(response['body'])\n","        \n","        self.properties_links_df.at[thisIndex,'done'] = True\n","        self.properties_links_df.to_csv(f'{self.raw_data_path}{os.sep}properties_links_df.csv',index=False)\n","\n","        print(f'Property {thisIndex+1} of {self.properties_links_df.shape[0]} dumped')\n","\n","    def create_dataset(self) -> None:\n","        '''\n","        Access the dumped html code saved as text files for all the properties and\n","        retrieve the properties' features from them. It creates a new class property \"dataset\"\n","        '''\n","        dumped_data_files = os.listdir('../properties')\n","        if not hasattr(self,'properties_links_df'):\n","            try:\n","                self.properties_links_df = pd.read_csv('../properties_links_df.csv')\n","            except:\n","                print('WARNING: Could not find properties_links_df.')\n","\n","        # if dataset exists then this is an update so skip the files already processed\n","        if hasattr(self,'dataset'):\n","            dumped_data_files = [x for x in dumped_data_files if not (self.dataset['id']==x).any()]\n","\n","        with concurrent.futures.ProcessPoolExecutor() as executor:\n","            executor.map(self._get_single_property_data,dumped_data_files)\n","\n","        self.dataset = pd.read_csv('../idealista_dataset.csv')\n","\n","    def _get_single_property_data(self,prop_dumped_data_file_name:str) -> None:\n","        '''\n","        Used to retrieve single property data with parallel processing\n","        \n","        args:\n","        - prop_dumped_data_file_name: name of the text file to retrieve the data from\n","\n","        '''\n","        property_id = prop_dumped_data_file_name\n","\n","        with open(f'../properties/{property_id}','r') as f:\n","            soup = BeautifulSoup(f.read(),'html.parser')\n","\n","        print(f'{property_id}: soup parsed')\n","\n","        if soup.select('#notFoundWithSuggestions'):\n","            print(f'{property_id}: this property has been removed')\n","            try:\n","                row_to_remove = self.properties_links_df[self.properties_links_df['property_link'].str.contains(property_id)]\n","                self.properties_links_df = self.properties_links_df.drop(row_to_remove.Index)\n","                self.properties_links_df.to_csv('../properties_links_df.csv',index=False)\n","            except:\n","                pass\n","            return None\n","\n","        try:\n","            utag_script = list(filter(lambda x: 'utag_data' in x.get_text(),soup.select('script')))[0]\n","            utag_data = json.loads(str(utag_script).split(';')[0].split(' ')[7])\n","        except:\n","            print(f'{property_id}: cannot retrieve data')\n","            return None\n","\n","        property_details = soup.select_one('div.details-property')\n","        property_data = {\n","            'id':utag_data['ad']['id'],\n","            'propertyType':soup.select_one('.main-info .typology').text.strip().lower(),\n","            'title':soup.select_one('.main-info .txt-body').text.strip().lower(),\n","            'description': soup.select_one('div.comment').text,\n","            'locationId':utag_data['ad']['address']['locationId'],\n","            'price':utag_data['ad']['price'],\n","            'size':utag_data['ad']['characteristics']['constructedArea'],\n","            'hasParking':utag_data['ad']['characteristics'].get('hasParking',0), # if not exist, get 0\n","            'roomNumber':utag_data['ad']['characteristics']['roomNumber'],\n","            'bathNumber':utag_data['ad']['characteristics']['bathNumber'],\n","            'hasSwimmingPool':utag_data['ad']['characteristics'].get('hasSwimmingPool',0), # if not exist, get 0\n","            'hasTerrace':utag_data['ad']['characteristics'].get('hasTerrace',0), # if not exist, get 0\n","            'hasGarden':utag_data['ad']['characteristics'].get('hasGarden',0), # if not exist, get 0\n","            'hasLift':utag_data['ad']['characteristics'].get('hasLift',0), # if not exist, get 0\n","            'hasAirco':1 if 'aire acondicionado' in property_details.text.strip().lower() else 0,\n","            'hasFittedWardrobes':1 if 'armario empotrado' in property_details.text.strip().lower().replace('s','') else 0,\n","            'isGoodCondition':utag_data['ad']['condition']['isGoodCondition'],\n","            'isNeedsRenovating':utag_data['ad']['condition']['isNeedsRenovating'],\n","            'isNewDevelopment':utag_data['ad']['condition']['isNewDevelopment'],\n","            'energyCertification':utag_data['ad']['energyCertification']['type'],\n","            'featureTags': [x.get_text().strip() for x in soup.select('.info-features-tags')]\n","        }\n","\n","        year_built = list(filter(lambda x: 'construido en' in x.get_text().lower(), property_details.select('li')))\n","        property_data['yearBuilt'] = year_built[0].get_text() if len(year_built)>0 else \"no info\"\n","        del year_built\n","\n","        orientation = list(filter(lambda x: 'orientación ' in x.get_text().lower(), property_details.select('li')))\n","        property_data['orientation'] = orientation[0].get_text() if len(orientation)>0 else \"no info\"\n","        del orientation\n","\n","        heatingData = list(filter(lambda x: 'calefacción' in x.get_text().lower(),property_details.select('li')))\n","        property_data['heatingType'] = heatingData[0].get_text() if heatingData else \"no info\"\n","        del heatingData\n","\n","        if property_data['propertyType'] == 'piso':\n","            info_features = soup.select('.info-features > span')\n","            if [x for x in info_features if \"interior\" in x.get_text()]:\n","                property_data['interiorExterior'] = \"interior\"\n","            elif [x for x in info_features if \"exterior\" in x.get_text()]:\n","                property_data['interiorExterior'] = \"exterior\"\n","            else:\n","                property_data['interiorExterior'] = \"no info\"\n","            floor_info = [x for x in info_features if re.search(\"bajo|sótano|planta\", x.get_text().lower())]\n","            if floor_info:\n","                property_data['floor'] = floor_info[0].select_one('span').get_text().lower().strip()\n","            else:\n","                property_data['floor'] = \"no info\"\n","        else:\n","            property_data['floor'] = property_data['interiorExterior'] = \"does not apply\"\n","        \n","        property_data_df = pd.DataFrame.from_dict(property_data,orient='index').T\n","        print(f'{property_id}: data converted to DF')\n","\n","        header = not os.path.exists('../idealista_dataset.csv')\n","        property_data_df.to_csv('../idealista_dataset.csv', mode='a', index=False, header=header)\n","\n","    def get_location_ids_mapper(self) -> dict:\n","        '''\n","        By default, every record in the dataset has de feature \"location id\" which\n","        is the website own id to identify the district. This function scrape and\n","        creates a new class property named \"location_ids_mapper\", a dict to map \n","        location ids to district's names\n","\n","        '''\n","        \n","        if os.path.exists('../location_ids.json'):\n","            with open('../location_ids.json','r') as f:\n","                location_ids_mapper = json.loads(f.read())\n","        else:\n","            response = self._proxy_requests(self._url)\n","            if response['statusCode'] != 200:\n","                raise f'Could not retrieve the location mapper'\n","            soup = BeautifulSoup(response['body'],'html.parser')\n","\n","            locations_list = [(x.get('data-location-id'),x.select_one('a').get_text()) for x in soup.select('.breadcrumb-dropdown-subitem-element-list')]\n","\n","            location_ids_mapper = {key:value for key,value in locations_list}\n","\n","            with open('../location_ids.json','w') as f:\n","                f.write(json.dumps(location_ids_mapper))\n","\n","        return location_ids_mapper\n","\n","    def full_scrape(self) -> None:\n","        '''\n","        Runs a full scrape of idelista properties. Firstly it will run get_areas_df(),\n","        then generate_properties_links_df() and get_properties_data(). It will then complete\n","        the dataset by running the get_location_ids_mapper() and applying the mapper to the \n","        dataset. Finally, it will export the dataset calling the file \"idealista_dataset.csv\"\n","\n","        Receives no arguments and returns None\n","        '''\n","        self.get_areas_df()\n","        \n","        print(f'Found {self.areas_df[\"n_houses\"].sum()} houses')\n","\n","        self.generate_properties_links_df()\n","\n","        self.properties_links_df = self.properties_links_df.drop_duplicates()\n","\n","        self.get_properties_data()\n","\n","        self.create_dataset()\n","\n","        mapper = self.get_location_ids_mapper()\n","        self.dataset['locationId'] = self.dataset['locationId'].apply(lambda x: \"-\".join(x.split(\"-\")[:8]))\n","        self.dataset['area_name'] = self.dataset['locationId'].map(mapper)\n","        self.dataset = self.dataset.drop('locationId',axis=1)\n","\n","        self.dataset.to_csv('../idealista_dataset.csv',index=False)"],"metadata":{"id":"hs3hWNTKFwX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = 'https://www.idealista.com/venta-viviendas/madrid-madrid/'\n","scraper = idealista_scraper(url, config_path, raw_data_path, clean_data_path)\n","\n","scraper.full_scrape()"],"metadata":{"id":"xBZyj4HEFuIj"},"execution_count":null,"outputs":[]}]}